{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practicando.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lskywolfll/TensorFlow/blob/master/Practicando.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2GgZ0gmSbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "a47b1e01-f3d6-44be-85f4-6bf05275e397"
      },
      "source": [
        "from matplotlib import animation\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Creamos los puntos de entrada de datos de nuestro grafo(mapa)\n",
        "# Crearemos los inicion de nuestros datos en el esquema u mapa donde estaran\n",
        "# Nuestras Capas\n",
        "\n",
        "#Creamos un esquema inicial de nuestra funcion, tipo de dato y la capa en caso \n",
        "#Que llegue a necesitar\n",
        "#Estos modelo de estructura de los datos iniciales dependen de lo que nosotros\n",
        "#Queramos hacer\n",
        "iX = tf.placeholder('float', shape = [None, X.shape[1]])\n",
        "iY = tf.placeholder('float', shape = [None])\n",
        "\n",
        "#creamos un mini arreglo con numero los cuales seran la cantidad de neuronas a\n",
        "#cada capa de nuestra red neuronal en base a su posicion\n",
        "# nn = [primera capa,segunda capa,tercera capa, cuarta capa]\n",
        "nn = [2, 16, 8, 1] # Numero de neuronas por capas\n",
        "lr = 0.01 # Ratio de aprendizaje para optimizar o rango de aprendizaje continuo\n",
        "\n",
        "# Primera capa\n",
        "W1 = tf.Variable(tf.random_normal([nn[0], nn[1]]), name = 'Weights_1')\n",
        "#Aqui indicamos a que capa en concreto se almacena\n",
        "b1 = tf.Variable(tf.random_normal([nn[1]]), name = 'bias_1')\n",
        "\n",
        "#Operacion de la primera capa\n",
        "#Consiste en usar el valor de entrada iX * el arreglo de parametros W1 + bias\n",
        "#Por ultimo esto se pasa por la funcion de activacion que implementemos\n",
        "\n",
        "#Primer aprendizaje con la funcion de activacion relu\n",
        "l1 = tf.nn.relu(tf.add(tf.matmul( iX , W1), b1))\n",
        "\n",
        "# segunda capa\n",
        "W2 = tf.Variable(tf.random_normal([nn[1], nn[2]]), name = 'Weights_2')\n",
        "#Aqui indicamos a que capa en concreto se almacena\n",
        "b2 = tf.Variable(tf.random_normal([nn[2]]), name = 'bias_2')\n",
        "\n",
        "#Operacion de la segunda capa\n",
        "#Consiste en usar el valor de entrada iX * el arreglo de parametros W1 + bias\n",
        "#Por ultimo esto se pasa por la funcion de activacion que implementemos\n",
        "\n",
        "#Primer aprendizaje con la funcion de activacion relu\n",
        "l2 = tf.nn.relu(tf.add(tf.matmul( iX , W2), b2))\n",
        "\n",
        "# Tercera capa\n",
        "W3 = tf.Variable(tf.random_normal([nn[2], nn[3]]), name = 'Weights_3')\n",
        "#Aqui indicamos a que capa en concreto se almacena\n",
        "b3 = tf.Variable(tf.random_normal([nn[3]]), name = 'bias_3')\n",
        "\n",
        "#Operacion de la tercera capa\n",
        "#Consiste en usar el valor de entrada iX * el arreglo de parametros W1 + bias\n",
        "#Por ultimo esto se pasa por la funcion de activacion que implementemos\n",
        "\n",
        "#Primer aprendizaje con la funcion de activacion sigmoide para que empieze desde\n",
        "# 0 en adelante siendo nuestro vector(punto) de predicciones en Y\n",
        "pY = tf.nn.sigmoid(tf.add(tf.matmul( l2 , W3), b3))[:,0]\n",
        "\n",
        "#Evaluacion de las predicciones\n",
        "#Error cuadratico medio\n",
        "#Aqui vendria usandose las estructura de nuestros datos(placeholder)\n",
        "#Recordar que los placeholder esperan un dato para que nosotros podamos utilizar\n",
        "loss = tf.losses.mean_squared_error(pY, iY)\n",
        "\n",
        "#Creacion del optimizador del entrenamiento para disminuir el coste de error\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minize(loss)\n",
        "\n",
        "# Cantidad de pasos para entrar a nuestra red neuronal\n",
        "n_steps = 1000\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  #Esto correra de manera aleatoria como definimos en las capas, para probar\n",
        "  #Con todas para asi ver si funciona y despues poco a poco optimizar\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  for step in range(n_steps):\n",
        "    #Podemos agregar datos con feed_dict que necesita un arreglo de datos inicial\n",
        "    # y un vector(punto) de salida\n",
        "    #Cada vez que se entrene se ira optimizando automaticamente ya que le indicamos\n",
        "    #usar optimizer(declarada anteriormente para optimizar progresivamente la red)\n",
        "    #Nos devolvera el resultado del optimizador\n",
        "    # Iniciamos todos los parametros de nuestra red y los contenedores de W y b\n",
        "    _, _loss, _pY = sess.run([optimizer, loss, pY], feed_dict={ iX : X, iY : Y})\n",
        "    \n",
        "    if step % 25 == 0:\n",
        "      #Calculo de precicion entre el la posicion real(vector) y las predicciones\n",
        "      acc = np.mean(np.round(_pY) == Y)\n",
        "      \n",
        "      print('Step', step, '/', ' - Loss = ', _loss, ' - Acc =', acc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2823fbf24830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Estos modelo de estructura de los datos iniciales dependen de lo que nosotros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Queramos hacer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0miX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0miY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    }
  ]
}